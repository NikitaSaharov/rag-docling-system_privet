#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–∞—É–∑–∞–º–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
"""

import os
import json
import requests
import hashlib
import time
from pathlib import Path

OLLAMA_URL = "http://ollama-docling:11434"
QDRANT_URL = "http://qdrant-docling:6333"
COLLECTION_NAME = "documents"

def get_embedding(text, model="nomic-embed-text", retries=3):
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(retries):
        try:
            response = requests.post(
                f"{OLLAMA_URL}/api/embeddings",
                json={"model": model, "prompt": text},
                timeout=60
            )
            response.raise_for_status()
            return response.json()["embedding"]
        except Exception as e:
            if attempt < retries - 1:
                print(f"    ‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 2}/{retries} —á–µ—Ä–µ–∑ 3 —Å–µ–∫...")
                time.sleep(3)
            else:
                print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")
                return None

def get_optimal_chunk_size(text):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    word_count = len(text.split())
    
    if word_count < 5000:
        # –ú–∞–ª–µ–Ω—å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è —Ñ–æ—Ä–º—É–ª
        return 300, 60
    elif word_count < 20000:
        # –°—Ä–µ–¥–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        return 350, 70
    else:
        # –ë–æ–ª—å—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        return 300, 60

def chunk_text(text, chunk_size=None, overlap=None):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º"""
    words = text.split()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
    if chunk_size is None or overlap is None:
        chunk_size, overlap = get_optimal_chunk_size(text)
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: {len(words)} —Å–ª–æ–≤")
        print(f"üîß –ê–≤—Ç–æ-–ø–æ–¥–±–æ—Ä: chunk_size={chunk_size}, overlap={overlap}")
    
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        if chunk:
            chunks.append(chunk)
    return chunks

def add_to_qdrant(chunk_id, embedding, text, metadata):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤–µ–∫—Ç–æ—Ä –≤ Qdrant"""
    try:
        response = requests.put(
            f"{QDRANT_URL}/collections/{COLLECTION_NAME}/points",
            json={
                "points": [{
                    "id": chunk_id,
                    "vector": embedding,
                    "payload": {
                        "text": text,
                        **metadata
                    }
                }]
            },
            timeout=30
        )
        response.raise_for_status()
        return True
    except Exception as e:
        return False

def process_file(file_path):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ —Å–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
    print(f"\n{'='*60}")
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path}")
    print(f"{'='*60}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    if not content.strip():
        print("‚ö†Ô∏è  –§–∞–π–ª –ø—É—Å—Ç–æ–π")
        return
    
    chunks = chunk_text(content)
    print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}\n")
    
    filename = Path(file_path).name
    success_count = 0
    
    for idx, chunk in enumerate(chunks):
        chunk_id = hashlib.md5(f"{filename}_{idx}".encode()).hexdigest()
        
        print(f"  [{idx+1}/{len(chunks)}] –ß–∞–Ω–∫ {idx+1}...", end=" ")
        
        embedding = get_embedding(chunk)
        
        if embedding:
            metadata = {
                "filename": filename,
                "chunk_index": idx,
                "total_chunks": len(chunks)
            }
            
            if add_to_qdrant(chunk_id, embedding, chunk, metadata):
                print("‚úÖ")
                success_count += 1
            else:
                print("‚ùå Qdrant")
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        if idx < len(chunks) - 1:
            time.sleep(1)
    
    print(f"\n‚ú® –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –£—Å–ø–µ—à–Ω–æ: {success_count}/{len(chunks)}\n")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python create_embeddings_slow.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É>")
        sys.exit(1)
    
    process_file(sys.argv[1])
