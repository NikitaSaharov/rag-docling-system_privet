#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —á–∞–Ω–∫–æ–≤
"""

import os
import json
import requests
import hashlib
import time
from pathlib import Path

OLLAMA_URL = "http://ollama-docling:11434"
QDRANT_URL = "http://qdrant-docling:6333"
COLLECTION_NAME = "documents"

def get_embedding(text, model="nomic-embed-text", retries=5):
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    import sys
    for attempt in range(retries):
        try:
            sys.stdout.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
            response = requests.post(
                f"{OLLAMA_URL}/api/embeddings",
                json={"model": model, "prompt": text},
                timeout=90  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
            )
            response.raise_for_status()
            sys.stdout.flush()
            return response.json()["embedding"]
        except Exception as e:
            sys.stdout.flush()
            if attempt < retries - 1:
                wait_time = (attempt + 1) * 3  # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ
                print(f"    ‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 2}/{retries} —á–µ—Ä–µ–∑ {wait_time} —Å–µ–∫...", flush=True)
                time.sleep(wait_time)
            else:
                print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫: {e}", flush=True)
                return None

def chunk_text(text, chunk_size=300, overlap=60):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        if chunk:
            chunks.append(chunk)
    return chunks

def get_existing_chunks(filename):
    """–ü–æ–ª—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤"""
    try:
        response = requests.post(
            f"{QDRANT_URL}/collections/{COLLECTION_NAME}/points/scroll",
            json={
                "limit": 10000,
                "with_payload": True,
                "filter": {
                    "must": [{
                        "key": "filename",
                        "match": {"value": filename}
                    }]
                }
            },
            timeout=10
        )
        response.raise_for_status()
        data = response.json()
        
        existing_indices = set()
        total_chunks = 0
        
        for point in data.get("result", {}).get("points", []):
            payload = point.get("payload", {})
            idx = payload.get("chunk_index")
            if idx is not None:
                existing_indices.add(idx)
            total = payload.get("total_chunks", 0)
            if total > total_chunks:
                total_chunks = total
        
        return existing_indices, total_chunks
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —á–∞–Ω–∫–æ–≤: {e}")
        return set(), 0

def add_to_qdrant(chunk_id, embedding, text, metadata):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤–µ–∫—Ç–æ—Ä –≤ Qdrant"""
    try:
        response = requests.put(
            f"{QDRANT_URL}/collections/{COLLECTION_NAME}/points",
            json={
                "points": [{
                    "id": chunk_id,
                    "vector": embedding,
                    "payload": {
                        "text": text,
                        **metadata
                    }
                }]
            },
            timeout=30
        )
        response.raise_for_status()
        return True
    except Exception as e:
        print(f"    –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ Qdrant: {e}")
        return False

def complete_missing_chunks(file_path):
    """–î–æ–¥–µ–ª—ã–≤–∞–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —á–∞–Ω–∫–∏"""
    import sys
    filename = Path(file_path).name
    
    print(f"\n{'='*60}", flush=True)
    print(f"üîß –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {filename}", flush=True)
    print(f"{'='*60}", flush=True)
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–∞–Ω–∫–∏
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —á–∞–Ω–∫–∞—Ö –∏–∑ Qdrant...", flush=True)
    existing_indices, total_chunks = get_existing_chunks(filename)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(existing_indices)} —á–∞–Ω–∫–æ–≤ –∏–∑ {total_chunks}", flush=True)
    
    if total_chunks == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ Qdrant")
        return
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    print(f"üìñ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path}...", flush=True)
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print("üî™ –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏...", flush=True)
    chunks = chunk_text(content)
    
    if len(chunks) != total_chunks:
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –≤ —Ñ–∞–π–ª–µ ({len(chunks)}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ ({total_chunks})")
        total_chunks = len(chunks)
    
    # –ù–∞—Ö–æ–¥–∏–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ
    all_indices = set(range(total_chunks))
    missing_indices = sorted(all_indices - existing_indices)
    
    if not missing_indices:
        print("‚úÖ –í—Å–µ —á–∞–Ω–∫–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
        return
    
    print(f"üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
    print(f"‚úÖ –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(existing_indices)}")
    print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—ë—Ç: {len(missing_indices)}")
    print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —á–∞–Ω–∫–æ–≤...\n")
    
    success_count = 0
    
    import sys
    for idx in missing_indices:
        if idx >= len(chunks):
            print(f"  ‚ö†Ô∏è  –ß–∞–Ω–∫ #{idx} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞", flush=True)
            continue
        
        chunk = chunks[idx]
        chunk_id = hashlib.md5(f"{filename}_{idx}".encode()).hexdigest()
        
        current_num = len(existing_indices) + success_count + 1
        print(f"  [{current_num}/{total_chunks}] –ß–∞–Ω–∫ #{idx}...", end=" ", flush=True)
        
        embedding = get_embedding(chunk)
        
        if embedding:
            metadata = {
                "filename": filename,
                "chunk_index": idx,
                "total_chunks": total_chunks
            }
            
            if add_to_qdrant(chunk_id, embedding, chunk, metadata):
                print("‚úÖ", flush=True)
                success_count += 1
            else:
                print("‚ùå Qdrant", flush=True)
        else:
            print("‚ùå Embedding", flush=True)
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        if idx < missing_indices[-1]:  # –ù–µ –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
            time.sleep(2)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞
    
    print(f"\n‚ú® –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —á–∞–Ω–∫–æ–≤: {success_count}/{len(missing_indices)}")
    print(f"üìä –ò—Ç–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(existing_indices) + success_count}/{total_chunks}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python complete_missing_chunks.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É>")
        sys.exit(1)
    
    complete_missing_chunks(sys.argv[1])
