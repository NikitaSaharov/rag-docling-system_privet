# Параллельная обработка запросов VectorStom

## ✅ Да, система поддерживает параллельные запросы!

### Архитектура
Система использует **Gunicorn** с несколькими worker-процессами для обработки одновременных запросов от разных пользователей.

---

## Текущая конфигурация

### Количество worker'ов: **4**
```dockerfile
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", ...]
```

**Это означает:**
- ✅ 4 пользователя могут **одновременно** получать ответы
- ✅ Каждый worker независим и обрабатывает свой запрос
- ✅ Нет блокировки - пока один запрос идет к DeepSeek API, другой может искать в Qdrant

---

## Производительность по конфигурациям

### C1-M2-D20 (1 vCPU, 2 ГБ RAM) - 4 worker'а
```
Параллельные пользователи: 3-5 без задержек
Максимум одновременно: 8-10 (с небольшими задержками)
Время ответа: 3-5 секунд на запрос
```

**Узкое место:** CPU при эмбеддингах от Ollama

**Рекомендация:** Для тестирования 5-10 пользователей - достаточно

---

### C4-M8-D120 (4 vCPU, 8 ГБ RAM) - 8 worker'ов
```
Параллельные пользователи: 10-15 без задержек
Максимум одновременно: 20-30
Время ответа: 2-4 секунды
```

Изменить количество worker'ов в Dockerfile:
```dockerfile
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "8", ...]
```

**Рекомендация:** Для продакшена 10-50 пользователей

---

## Как работает параллельность

### Без Gunicorn (стандартный Flask)
```
User 1: Запрос → [Flask] → 5 сек → Ответ
User 2: Запрос → ЖДЁТ User 1 → [Flask] → 5 сек → Ответ
User 3: Запрос → ЖДЁТ User 1 + User 2 → [Flask] → ...
```
❌ **Последовательная обработка** - второй пользователь ждет завершения первого

### С Gunicorn (текущая система)
```
User 1: Запрос → [Worker 1] → 5 сек → Ответ
User 2: Запрос → [Worker 2] → 5 сек → Ответ  (одновременно!)
User 3: Запрос → [Worker 3] → 5 сек → Ответ  (одновременно!)
User 4: Запрос → [Worker 4] → 5 сек → Ответ  (одновременно!)
User 5: Запрос → ЖДЁТ освобождения Worker'а → ...
```
✅ **Параллельная обработка** - до 4 запросов одновременно

---

## Компоненты системы и параллельность

### 1. Flask + Gunicorn (webapp)
- ✅ **4 worker'а** - обрабатывают 4 запроса параллельно
- Каждый worker - отдельный процесс Python

### 2. Qdrant (векторная БД)
- ✅ **Полностью параллельный** - может обслуживать десятки запросов одновременно
- Быстрый поиск (~50-100 мс)

### 3. Ollama (эмбеддинги)
- ⚠️ **Ограниченная параллельность** - зависит от CPU
- На 1 vCPU: 2-3 запроса параллельно
- На 4 vCPU: 8-10 запросов параллельно

### 4. DeepSeek API (Polza.ai)
- ✅ **Полностью параллельный** - внешний сервис, нет ограничений
- Время ответа: 2-3 секунды

---

## Тестирование параллельности

### Локально (перед развертыванием)
```bash
# Запустите сервер
docker compose -f docker-compose.simple-prod.yml up -d

# Откройте 3-4 вкладки браузера с http://localhost
# Отправьте вопросы одновременно из всех вкладок

# Проверьте логи worker'ов
docker logs -f vectorstom-webapp
```

Вы увидите:
```
[2026-01-14 20:30:01] [1] [INFO] Worker 1: Processing query "Что такое нормочас?"
[2026-01-14 20:30:01] [2] [INFO] Worker 2: Processing query "Как рассчитать ВВ?"
[2026-01-14 20:30:02] [3] [INFO] Worker 3: Processing query "Коэффициент загрузки"
```

### На продакшене
```bash
# Отправьте ссылку 3-5 коллегам
# Попросите задать вопросы одновременно
# Проверьте время ответа - должно быть ~3-5 сек для всех
```

---

## Оптимизация для большого количества пользователей

### Вариант 1: Увеличить количество worker'ов
**Правило:** `workers = (CPU_cores * 2) + 1`

Для C4-M8 (4 ядра):
```dockerfile
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "9", ...]
```

### Вариант 2: Использовать worker class "gevent"
Асинхронные worker'ы - больше одновременных запросов на меньших ресурсах:
```dockerfile
CMD ["gunicorn", "--bind", "0.0.0.0:5000", 
     "--workers", "4", 
     "--worker-class", "gevent",
     "--worker-connections", "1000",
     "app:app"]
```

Требует добавить в requirements.txt:
```
gevent
```

**Производительность:** 50-100 одновременных пользователей на C4-M8

### Вариант 3: Кеширование (Redis)
Кешировать частые вопросы:
```python
# Если вопрос уже был задан - вернуть из кеша
# Время ответа: ~50 мс вместо 5 секунд
```

---

## Мониторинг параллельности

### Количество активных worker'ов
```bash
docker exec vectorstom-webapp ps aux
# Увидите 4 процесса gunicorn
```

### Нагрузка на систему
```bash
docker stats vectorstom-webapp
# CPU usage > 90% = нужно больше ядер
# Memory usage > 80% = нужно больше RAM
```

### Логи параллельных запросов
```bash
docker logs -f vectorstom-webapp | grep "Processing"
# Увидите одновременные запросы от разных worker'ов
```

---

## Рекомендации по масштабированию

| Пользователей | Конфигурация | Worker'ы | Стоимость |
|---------------|--------------|----------|-----------|
| 5-10 одновременно | C1-M2-D20 | 4 | 1 100 ₽/мес |
| 10-20 одновременно | C4-M8-D120 | 8 | 4 880 ₽/мес |
| 20-50 одновременно | C8-M16-D120 | 16 | 8 320 ₽/мес |
| 50+ одновременно | Load balancer + 2 сервера | 16×2 | ~18 000 ₽/мес |

---

## Часто задаваемые вопросы

### Q: Сколько пользователей могут использовать систему одновременно на C1-M2?
**A:** 3-5 без задержек, до 10 с небольшими задержками (1-2 секунды)

### Q: Что будет, если 10 человек одновременно зададут вопрос на C1-M2?
**A:** 
- Первые 4 - получат ответ за 3-5 секунд
- Следующие 4 - получат ответ за 6-10 секунд (ждут освобождения worker'а)
- Последние 2 - получат ответ за 9-15 секунд

### Q: Как проверить, что параллельность работает?
**A:** Откройте 2-3 вкладки браузера, задайте вопросы одновременно. Все должны получить ответ примерно в одно время (~5 сек), а не последовательно (5→10→15 сек)

### Q: Нужно ли что-то настраивать дополнительно?
**A:** Нет! Gunicorn уже настроен в Dockerfile. После развертывания параллельность работает автоматически.

---

## Итог

✅ **Система готова к параллельной работе из коробки**
- 4 worker'а обрабатывают запросы одновременно
- Qdrant и DeepSeek API полностью параллельные
- На C1-M2-D20: комфортно работают 5-10 пользователей
- При необходимости легко масштабируется до C4-M8

**Тестирование:** Развернуть → Дать ссылку 3-5 коллегам → Проверить время ответа
